{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSVs\n",
    "import load_data as ld\n",
    "\n",
    "# To load train and test data\n",
    "# No need to repeat loading data if just need to change training parameters\n",
    "in_height = 96\n",
    "in_width = 96\n",
    "num_rows = 500\n",
    "\n",
    "inputs = ld.read_data('train.csv', in_height, in_width, nrows=num_rows)\n",
    "labels = ld.read_label('train_label.csv', nrows=num_rows)\n",
    "x_predict = ld.read_data('test.csv', in_height, in_width, nrows=num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split inputs for training and testing\n",
    "import numpy as np\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "np.random.seed(0)\n",
    "mask = np.random.rand(inputs.shape[0]) <= train_ratio\n",
    "\n",
    "x_train = inputs[mask]\n",
    "y_train = labels[mask]\n",
    "x_test = inputs[~mask]\n",
    "y_test = labels[~mask]\n",
    "\n",
    "print(\"x_train.shape\", x_train.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"x_test.shape\", x_test.shape)\n",
    "print(\"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from conv_net import conv_net\n",
    "from le_net import le_net\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_classes = 2  # total classes (0 or 1)\n",
    "dropout = 0.25  # Dropout, probability to drop a unit\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    # LeNet:\n",
    "    logits_train = le_net(features, in_height, in_width, num_classes, dropout, reuse=False, is_training=True)\n",
    "    logits_test = le_net(features, in_height, in_width, num_classes, dropout, reuse=True, is_training=False)\n",
    "\n",
    "    # ConvNet:\n",
    "    # logits_train = conv_net(features, in_height, in_width, num_classes, dropout, reuse=False, is_training=True)\n",
    "    # logits_test = conv_net(features, in_height, in_width, num_classes, dropout, reuse=True, is_training=False)\n",
    "\n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "\n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_probas)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                  global_step=tf.train.get_global_step())\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "    print(\"Current accuracy of model\", acc_op)\n",
    "\n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=pred_classes,\n",
    "        loss=loss_op,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "train_epoch = 5\n",
    "batch_size = 128\n",
    "num_steps = 50\n",
    "ckpt_steps = 10\n",
    "max_ckpt = 50\n",
    "\n",
    "rc = tf.estimator.RunConfig(model_dir = \"./model\", keep_checkpoint_max=max_ckpt, save_checkpoints_steps=ckpt_steps)\n",
    "model = tf.estimator.Estimator(model_fn, config=rc)\n",
    "# model = tf.estimator.Estimator(model_fn)\n",
    "\n",
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x= {'file': x_train}, y=y_train,\n",
    "    batch_size=batch_size, num_epochs=train_epoch, shuffle=True)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'file': x_test}, y=y_test,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Use the Estimator 'evaluate' method\n",
    "e = model.evaluate(input_fn)\n",
    "total_steps = e['global_step']\n",
    "print(\"global_step:\", e['global_step'])\n",
    "print('accuracy = ', e['accuracy'], \"loss = \", e['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate checkpoints\n",
    "import pandas as pd\n",
    "\n",
    "print('total_steps = ', total_steps)\n",
    "\n",
    "total_ckpts = total_steps//ckpt_steps\n",
    "eval_results = np.zeros((total_ckpts, 3))\n",
    "\n",
    "for i in range(0, total_ckpts):\n",
    "    j = (i + 1) * ckpt_steps + 1\n",
    "    ckpt_path = './model/model.ckpt-' + str(j)\n",
    "    print(ckpt_path)\n",
    "    e = model.evaluate(input_fn, checkpoint_path=ckpt_path)\n",
    "    eval_results[i,:] = [j, e['accuracy'], e['loss']]\n",
    "    \n",
    "df = pd.DataFrame(eval_results)\n",
    "header = [\"step\",\"accuracy\", \"loss\"]\n",
    "df.to_csv('./eval_ckpts.csv', header=header, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'file': x_predict},\n",
    "    batch_size=batch_size, num_epochs=1, shuffle=False)\n",
    "\n",
    "results = model.predict(input_fn=predict_input_fn)\n",
    "\n",
    "i = 0\n",
    "with open('result.csv', 'w') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile,)\n",
    "    csv_writer.writerow([\"sample_id\", \"malware\"])\n",
    "    for result in results:\n",
    "        csv_writer.writerow([i, result[1]])\n",
    "        i = i+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
